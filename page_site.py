# ==============================
# page_site.py (PART 1 / 2)
# ==============================
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import streamlit as st
import plotly.graph_objects as go
import joblib
import numpy as np
from datetime import datetime
from zoneinfo import ZoneInfo

# ------------------------------
# Optional deps
# ------------------------------
try:
    import shap  # type: ignore
    _HAS_SHAP = True
except Exception:
    shap = None  # type: ignore
    _HAS_SHAP = False

try:
    from openai import OpenAI  # type: ignore
    _HAS_OPENAI = True
except Exception:
    OpenAI = None  # type: ignore
    _HAS_OPENAI = False

from ui_components import apply_plotly_card_style
from app_common import (
    PLOTLY_CFG,
    set_query_params_safe,
    normalize_columns,
    to_datetime_safe,
)

# ------------------------------
# Constants
# ------------------------------
TARGETS = {"O3": "o3_mean", "NO2": "no2_mean", "CO": "co_mean", "SO2": "so2_mean"}

MODEL_PATHS = {
    "O3": "models/model_o3.joblib",
    "NO2": "models/model_no2.joblib",
    "CO": "models/model_co.joblib",
    "SO2": "models/model_so2.joblib",
}

FUTURE_DATA_PATH_DEFAULT = "future_input_2024_01_01_to_07_all_sites_MODELREADY.csv"
FUTURE_DATA_PATTERNS = ["future_input_*MODELREADY*.csv", "future_input_*.csv"]

SPIKE_THRESHOLD = 0.30
P90_Q = 0.90


# ------------------------------
# Dummy model (fallback)
# ------------------------------
class DummySpikeModel:
    _is_dummy = True

    def __init__(self, feature_names: Optional[List[str]] = None):
        self.feature_name_ = feature_names or []

    def predict_proba(self, X):
        n = 0 if X is None else len(X)
        # always negative
        return np.column_stack([np.ones(n), np.zeros(n)])


def _get_model_features(model) -> List[str]:
    feats = getattr(model, "feature_name_", None)
    if feats is None:
        feats = getattr(model, "feature_names_in_", None)
    if feats is None:
        return []
    return list(feats)


def _resolve_existing_path(path_str: str, patterns: List[str]) -> Path:
    base_file = Path(__file__).resolve().parent
    search_dirs = [
        Path.cwd(),
        base_file,
        base_file.parent,
        base_file / "data",
        base_file.parent / "data",
        Path.cwd() / "data",
    ]

    if path_str:
        p = Path(path_str)
        if p.is_file():
            return p
        for d in search_dirs:
            cand = d / path_str
            if cand.is_file():
                return cand

    for d in search_dirs:
        if not d.exists():
            continue
        for pat in patterns:
            hits = sorted([p for p in d.glob(pat) if p.is_file()])
            if hits:
                return hits[0]

    raise FileNotFoundError(
        f"ë¯¸ë˜ ì…ë ¥ ë°ì´í„°(CSV)ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì…ë ¥='{path_str}'. "
        f"íƒìƒ‰ í´ë”={[str(x) for x in search_dirs]}"
    )


@st.cache_data(show_spinner=False)
def load_future(path_str: str) -> pd.DataFrame:
    p = _resolve_existing_path(path_str, FUTURE_DATA_PATTERNS)
    df = pd.read_csv(p)
    df = normalize_columns(df)
    if "date" not in df.columns or "site" not in df.columns:
        raise ValueError("ë¯¸ë˜ ì…ë ¥ CSVì— date, site ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    df["date"] = to_datetime_safe(df["date"])
    df = df.dropna(subset=["date", "site"]).copy()
    return df.sort_values(["site", "date"]).reset_index(drop=True)


@st.cache_resource(show_spinner=False)
def load_spike_models_safe() -> Tuple[Dict[str, object], List[str]]:
    models: Dict[str, object] = {}
    errors: List[str] = []
    for k, path in MODEL_PATHS.items():
        try:
            models[k] = joblib.load(path)
        except Exception as e:
            models[k] = DummySpikeModel()
            errors.append(f"{k}: {type(e).__name__}")
    return models, errors


# âœ… SHAP ìºì‹œ í‚¤ëŠ” model_keyë§Œ hash, ëª¨ë¸ ê°ì²´ëŠ” í•´ì‹± ì œì™¸
@st.cache_resource(show_spinner=False)
def get_shap_explainer(model_key: str, _model):
    if not _HAS_SHAP:
        return None
    if getattr(_model, "_is_dummy", False):
        return None
    try:
        return shap.TreeExplainer(_model)  # type: ignore
    except Exception:
        return None


@st.cache_resource(show_spinner=False)
def get_openai_client():
    if not _HAS_OPENAI:
        raise RuntimeError("openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    api_key = st.secrets.get("OPENAI_API_KEY", None)
    if not api_key:
        raise RuntimeError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return OpenAI(api_key=api_key)  # type: ignore


def get_openai_model_name() -> str:
    return st.secrets.get("OPENAI_MODEL", "gpt-4.1-mini")


def ensure_required_features(
    df_future_site: pd.DataFrame,
    df_site_hist: pd.DataFrame,
    required_features: List[str],
) -> Tuple[pd.DataFrame, Dict[str, str]]:
    """
    ë¯¸ë˜ ì…ë ¥(df_future_site)ì— ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” í”¼ì²˜(required_features)ê°€ ì—†ìœ¼ë©´
    - ë¯¸ë˜ ì…ë ¥ ë‚´ alias(ëŒ€ì†Œë¬¸ì/ì •ê·œí™”)ë¡œ ì±„ìš°ê±°ë‚˜
    - ê³¼ê±° df_site_histì˜ ë§ˆì§€ë§‰ ìœ íš¨ê°’ìœ¼ë¡œ ìƒìˆ˜ ì±„ìš°ê±°ë‚˜
    - ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
    """
    out = df_future_site.copy()
    filled_info: Dict[str, str] = {}

    fut_map = {c.lower(): c for c in out.columns}
    hist_map = {c.lower(): c for c in df_site_hist.columns}

    def _last_nonnull_value(df_: pd.DataFrame, col_: str) -> float:
        s = pd.to_numeric(df_[col_], errors="coerce")
        if s.notna().any():
            return float(s.dropna().iloc[-1])
        return 0.0

    for feat in required_features:
        if feat in out.columns:
            continue

        key = feat.lower()

        if key in fut_map:
            src = fut_map[key]
            out[feat] = out[src]
            filled_info[feat] = f"alias_from_future:{src}"
            continue

        if key in hist_map:
            src = hist_map[key]
            out[feat] = _last_nonnull_value(df_site_hist, src)
            filled_info[feat] = f"constant_from_hist:{src}"
            continue

        out[feat] = 0.0
        filled_info[feat] = "default:0"

    # numeric + fillna
    for feat in required_features:
        out[feat] = pd.to_numeric(out[feat], errors="coerce")
        if out[feat].isna().all():
            out[feat] = 0.0
        else:
            out[feat] = out[feat].fillna(out[feat].median())

    return out, filled_info


WEATHER_FEATURE_MEANING = {
    "wind_speed": ("ì €í’ì†", "ê³ í’ì†"),
    "pressure_pa": ("ê³ ê¸°ì•• ì •ì²´", "ê¸°ì•• í˜¼í•©"),
    "temp_c": ("ê³ ì˜¨", "ì €ì˜¨"),
}
# SHAP ì¡°ê±´ â†’ ì‹¤ì œ ê¸°ìƒ ì»¬ëŸ¼ ë§¤í•‘
SHAP_REASON_TO_COLUMN = {
    "í’ì† ì¡°ê±´": ("wind_speed", "í’ì† (m/s)"),
    "ê¸°ì˜¨ ì¡°ê±´": ("temp_c", "ê¸°ì˜¨ (Â°C)"),
    "ê¸°ì•• ì¡°ê±´": ("pressure_pa", "ê¸°ì•• (Pa)"),
    "ìŠµë„ ì¡°ê±´": ("humidity", "ìŠµë„ (%)"),
    "ì¼ì‚¬ ì¡°ê±´": ("solar_radiation", "ì¼ì‚¬ëŸ‰"),
}



def explain_weather_keyword(model_key: str, model, X_row: pd.DataFrame) -> Optional[str]:
    """
    SHAPì—ì„œ |ê°’| í° í”¼ì²˜ë“¤ ì¤‘ WEATHER_FEATURE_MEANINGì— í•´ë‹¹í•˜ëŠ” ê²Œ ìˆìœ¼ë©´
    ë°©í–¥(+, -)ì— ë”°ë¼ ë¬¸êµ¬ ë°˜í™˜
    """
    explainer = get_shap_explainer(model_key, model)
    if explainer is None:
        return None
    try:
        sv = explainer(X_row, check_additivity=False)
        ranked = sorted(
            zip(X_row.columns, sv.values[0]),
            key=lambda x: abs(x[1]),
            reverse=True,
        )
        for feat, val in ranked:
            fkey = str(feat).lower()
            if fkey in WEATHER_FEATURE_MEANING:
                return WEATHER_FEATURE_MEANING[fkey][0 if val > 0 else 1]
    except Exception:
        return None
    return None

# ===============================
# Event-level SHAP explanation
# ===============================
WEATHER_FEATURE_GROUPS = {
    "wind": "í’ì† ì¡°ê±´",
    "temp": "ê¸°ì˜¨ ì¡°ê±´",
    "pressure": "ê¸°ì•• ì¡°ê±´",
    "humidity": "ìŠµë„ ì¡°ê±´",
    "solar": "ì¼ì‚¬ ì¡°ê±´",
}
# ===============================
# Spatial context summarizer
# ===============================
def summarize_spatial_context(row: pd.Series) -> dict:
    # ë„ì‹œí™” ìˆ˜ì¤€
    if row["impervious_pct"] > 60:
        urban_level = "ê³ ë„ì‹œí™”"
    elif row["impervious_pct"] > 40:
        urban_level = "ì¤‘ê°„ ë„ì‹œí™”"
    else:
        urban_level = "ì €ë„ì‹œí™”"

    # ë„ì‹œ í† ì§€í”¼ë³µ
    if row["urban_landcover_pct"] > 0.9:
        urban_cover = "ë„ì‹œ í† ì§€í”¼ë³µ ì§€ë°°"
    else:
        urban_cover = "í˜¼í•© í† ì§€í”¼ë³µ"

    # ê³ ë„
    if row["elevation_mean"] > 500:
        elevation = "ê³ ì§€ëŒ€"
    elif row["elevation_mean"] > 100:
        elevation = "ì¤‘ê°„ ê³ ë„"
    else:
        elevation = "ì €ì§€ëŒ€"

    # ì¸ê³µ êµ¬ì¡° / ë…¹ì§€ (ì»¬ëŸ¼ ì—†ìœ¼ë©´ ì¤‘ë¦½ ì²˜ë¦¬)
    ndbi = row.get("NDBI_mean", np.nan)
    ndvi = row.get("NDVI_mean", np.nan)

    if pd.notna(ndbi):
        built_env = "ì¸ê³µ êµ¬ì¡° ìš°ì„¸" if ndbi > 0 else "ìì—°Â·í˜¼í•© êµ¬ì¡°"
    else:
        built_env = "êµ¬ì¡° ì •ë³´ ë¶€ì¡±"

    if pd.notna(ndvi):
        green_env = "ë…¹ì§€ ë¶€ì¡±" if ndvi < 0.2 else "ì¤‘ê°„ ì´ìƒ ë…¹ì§€"
    else:
        green_env = "ë…¹ì§€ ì •ë³´ ë¶€ì¡±"

    return {
        "urban_level": urban_level,
        "urban_cover": urban_cover,
        "elevation": elevation,
        "built_env": built_env,
        "green_env": green_env,
    }


def explain_weather_keyword_event(
    model_key: str,
    model,
    X_all: pd.DataFrame,
    dates: pd.Series,
    center_date: pd.Timestamp,
    window: int = 1,
) -> Optional[str]:
    """
    í•˜ë£¨ê°€ ì•„ë‹Œ ì´ë²¤íŠ¸(Â±windowì¼) ê¸°ì¤€ SHAP í‰ê· ìœ¼ë¡œ
    'ëª¨ë¸ íŒë‹¨ì— ê¸°ì—¬í•œ ê¸°ìƒ ê·¸ë£¹'ì„ ë°˜í™˜
    """
    explainer = get_shap_explainer(model_key, model)
    if explainer is None:
        return None

    mask = (
        (dates >= center_date - pd.Timedelta(days=window)) &
        (dates <= center_date + pd.Timedelta(days=window))
    )
    X_evt = X_all.loc[mask]

    if X_evt.empty:
        return None

    try:
        sv = explainer(X_evt, check_additivity=False)
        mean_shap = np.mean(sv.values, axis=0)
    except Exception:
        return None

    ranked = sorted(
        zip(X_evt.columns, mean_shap),
        key=lambda x: abs(x[1]),
        reverse=True,
    )

    for feat, _ in ranked:
        fkey = feat.lower()
        for group_key, group_name in WEATHER_FEATURE_GROUPS.items():
            if group_key in fkey:
                return group_name

    return None


def _top_right_quick_widgets(anchor_date: pd.Timestamp):
    now = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y-%m-%d %H:%M")
    c1, c2, c3 = st.columns([1, 1, 1], gap="small")
    with c1:
        st.toggle("ğŸŒ™", key="ui_dark_mode", help="ë‹¤í¬/ë¼ì´íŠ¸ ëª¨ë“œ")
    with c2:
        st.toggle("ğŸ”•", key="mute_alerts", help="ì•Œë¦¼ ìŒì†Œê±°")
    with c3:
        if st.button("âŸ³", use_container_width=True, help="ì¦‰ì‹œ ìƒˆë¡œê³ ì¹¨"):
            st.rerun()
    st.caption(f"ğŸ•’ {now} | ê¸°ì¤€ì¼: {anchor_date.date()}")

# ==============================
# page_site.py (PART 2 / 2)
# ==============================

def build_ts_figure(
    pred: pd.DataFrame,
    df_all: pd.DataFrame,
    y_col: str,
    anchor: pd.Timestamp,
    spike_days: List[pd.Timestamp],
    show_5months: bool,
    view_range: Optional[Tuple[pd.Timestamp, pd.Timestamp]] = None,
) -> go.Figure:
    fig = go.Figure()
    pred = pred.copy()
    pred["date"] = pd.to_datetime(pred["date"])


    hist = pred[pred["date"] <= anchor]

    fig.add_trace(
        go.Scatter(
            x=hist["date"],
            y=hist["y"],
            name="Observed",
            line=dict(color="gray"),
        )
    )
    fig.add_trace(
        go.Scatter(
            x=pred["date"],
            y=pred["yhat"],
            name="Prophet",
            line=dict(color="#1f77b4", dash="dash"),
        )
    )

    if y_col in df_all.columns:
        fig.add_hline(
            y=float(df_all[y_col].quantile(P90_Q)),
            line=dict(color="orange", dash="dot"),
        )

    for d in spike_days:
        d0 = pd.to_datetime(d)
        fig.add_vrect(
            x0=d0,
            x1=d0 + pd.Timedelta(days=1),
            fillcolor="rgba(255,0,0,0.25)",
            line_width=0,
        )

    if view_range is not None:
        x0, x1 = view_range
        fig.update_xaxes(range=[x0, x1])

    fig.update_layout(height=255, margin=dict(l=8, r=8, t=20, b=8), uirevision=True)
    return apply_plotly_card_style(fig)


REPORT_SCHEMA_KEYS = [
    ("ìš”ì•½", "executive_summary"),
    ("í˜„ì¬ ë¦¬ìŠ¤í¬ ìƒíƒœ", "current_risk_status"),
    ("í•µì‹¬ ì›ì¸", "key_drivers"),
    ("7ì¼ ì „ë§", "seven_day_outlook"),
    ("ê¶Œê³  ì¡°ì¹˜", "recommended_actions"),
    ("ë¹„ê³  ë° í•œê³„", "notes_limitations"),
]


@st.cache_data(show_spinner=False)
def get_ts_figure_cached(
    site: str,
    y_col: str,
    show_5months: bool,
    view_start_iso: str,
    view_end_iso: str,
    anchor_iso: str,
    spike_days_iso: Tuple[str, ...],
):
    """
    Plotly figure ìºì‹±:
    - í•´ì‹œ ê°€ëŠ¥í•œ ê°’(ë¬¸ìì—´/tuple)ë§Œ ë°›ëŠ”ë‹¤.
    """
    pred = st.session_state["ALL_PROPHET"].get((site, y_col))
    if pred is None:
        return None

    df_all = st.session_state["DF_ALL"]
    anchor = pd.to_datetime(anchor_iso)
    view_range = (pd.to_datetime(view_start_iso), pd.to_datetime(view_end_iso))
    spike_days = [pd.to_datetime(x) for x in spike_days_iso]

    fig = build_ts_figure(
        pred=pred,
        df_all=df_all,
        y_col=y_col,
        anchor=anchor,
        spike_days=spike_days,
        show_5months=show_5months,
        view_range=view_range,
    )
    return fig


def build_llm_payload(
    site: str,
    anchor: pd.Timestamp,
    horizon: int,
    pollutant_summaries: List[dict],
    style_hint: str,
    spatial_context: dict,
) -> List[dict]:
    sys = (
        "ë„ˆëŠ” ëŒ€ê¸°í™˜ê²½ ìš´ì˜ê´€ë¦¬ì ë³´ì¡° AIë‹¤. "
        "ì•„ë˜ ì…ë ¥ì„ ê¸°ë°˜ìœ¼ë¡œ ìš´ì˜ììš© ë³´ê³ ì„œë¥¼ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë¼. "
        "ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ì´ì–´ì•¼ í•˜ë©°,"
        "ëª¨ë“  valueëŠ” í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
        "ì˜ì–´ ì‚¬ìš©ì€ ê³ ìœ ëª…ì‚¬(ë‹¨ìœ„, ê¸°í˜¸ ë“±)ë¥¼ ì œì™¸í•˜ê³  ê¸ˆì§€í•œë‹¤."
        "ê´€ì¸¡ì†Œì˜ ê³µê°„ì  íŠ¹ì„±(spatial_context)ì„ ê¸°ìƒ ìš”ì¸ê³¼ í•¨ê»˜ ë°˜ë“œì‹œ ì¢…í•©ì ìœ¼ë¡œ í•´ì„í•´ë¼."
    )
    user = {
        "site": site,
        "anchor_date": str(anchor.date()),
        "horizon_days": horizon,
        "pollutant_summaries": pollutant_summaries,
        "spatial_context": spatial_context,
        "style_hint": style_hint,
        "required_json_keys": [k for _, k in REPORT_SCHEMA_KEYS],
    }
    return [
        {"role": "system", "content": sys},
        {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
    ]


def generate_report_from_llm(messages: List[dict]) -> str:
    client = get_openai_client()
    model = get_openai_model_name()
    resp = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
    )
    return resp.choices[0].message.content or ""


def safe_parse_report_json(text: str) -> Dict[str, str]:
    try:
        obj = json.loads(text)
        if isinstance(obj, dict):
            return {k: str(v) for k, v in obj.items()}
    except Exception:
        pass
    return {"executive_summary": text}


def render_page2(
    df_all: pd.DataFrame,
    site: str,
    target: str,
    anchor: pd.Timestamp,
    horizon: int,
    interval_width: float,
    weather_fc,
    thr_config: dict,
    ALL_PROPHET: dict,
):
    # ------------------------------
    # CSS (ì›ë³¸ ìœ ì§€)
    # ------------------------------
    st.markdown(
        """
        <style>
        div.block-container { padding-top: 0.8rem; padding-bottom: 1.5rem; }
        section.main > div { padding-top: 0.8rem !important; }
        header { margin-bottom: 0rem !important; }
        </style>
        """,
        unsafe_allow_html=True,
    )

    # ìºì‹œ í•¨ìˆ˜ë“¤ì´ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ì €ì¥
    st.session_state["DF_ALL"] = df_all
    st.session_state["ALL_PROPHET"] = ALL_PROPHET

    # ------------------------------
    # Top header (ì›ë³¸ ìœ ì§€)
    # ------------------------------
    P2_TOP_RIGHT_BTN_MARGIN_PX = 20

    h1, h2 = st.columns([8, 2], gap="small")
    with h1:
        P2_TITLE_FONT_PX = 70
        st.markdown(
            f"""
            <div style="
              font-size:{P2_TITLE_FONT_PX}px;
              font-weight:800;
              margin-top:0px;
              margin-bottom:0px;
            ">
                {site}
            </div>
            """,
            unsafe_allow_html=True,
        )

    with h2:
        st.markdown(
            f"<div style='margin-top:{P2_TOP_RIGHT_BTN_MARGIN_PX}px;'></div>",
            unsafe_allow_html=True,
        )
        _top_right_quick_widgets(anchor)

        if st.button("ê´€ì¸¡ì†Œ ì„ íƒ", use_container_width=True, key="btn_back_overview"):
            set_query_params_safe(page="overview")
            st.rerun()

    # ------------------------------
    # Data
    # ------------------------------
    df_site = df_all[df_all["site"].astype(str) == str(site)].sort_values("date").copy()
    # ê³µê°„ ìš”ì•½ìš©: ê´€ì¸¡ì†Œ ëŒ€í‘œ 1í–‰ (ê³µê°„ ë³€ìˆ˜ëŠ” ì‹œê°„ì— ë”°ë¼ ë³€í•˜ì§€ ì•ŠìŒ)
    if not df_site.empty and "site-cluster" in df_site.columns:
        site_cluster = str(df_site.iloc[-1]["site-cluster"])
    else:
        site_cluster = "moderate"

    if df_site.empty:
        spatial_context = {}
    else:
        spatial_row = df_site.iloc[-1]
        spatial_context = summarize_spatial_context(spatial_row)

    # ------------------------------
    # (A) SPIKE_DF ìš”ì•½ (í‘œì‹œìš©) - í† ê¸€ ì—†ìŒ, í•­ìƒ í‘œì‹œ
    # ------------------------------
    spike_df = st.session_state.get("SPIKE_DF")
    site_spike_row = (
        spike_df[spike_df["site"].astype(str) == str(site)]
        if spike_df is not None
        else pd.DataFrame()
    )
    if not site_spike_row.empty:
        n_spike_summary = int(site_spike_row.iloc[0].get("spike_days", 0))
        priority_summary = str(site_spike_row.iloc[0].get("priority", "LOW"))
    else:
        n_spike_summary = 0
        priority_summary = "LOW"

    # ------------------------------
    # (B) ìƒì„¸ ìŠ¤íŒŒì´í¬/SHAP (ë¯¸ë˜ì…ë ¥+ëª¨ë¸ ìˆìœ¼ë©´ ìë™, ì—†ìœ¼ë©´ graceful)
    # âœ… ì‚¬ì´ë“œë°” í† ê¸€(spike_enabled) ì ˆëŒ€ ì‚¬ìš© ì•ˆ í•¨
    # ------------------------------
    df_future_site_filled = pd.DataFrame()
    models: Dict[str, object] = {k: DummySpikeModel() for k in TARGETS.keys()}
    spike_detail_available = False
    detail_warn: Optional[str] = None

    try:
        models, model_errors = load_spike_models_safe()
        df_future = load_future(st.session_state.get("future_input_path", FUTURE_DATA_PATH_DEFAULT))
        df_future_site = (
            df_future[df_future["site"].astype(str) == str(site)]
            .copy()
            .sort_values("date")
        )

        required_union: List[str] = []
        for m in models.values():
            required_union += _get_model_features(m)

        # unique preserve order
        seen = set()
        required_union = [x for x in required_union if not (x in seen or seen.add(x))]

        if not df_future_site.empty and required_union:
            df_future_site_filled, _ = ensure_required_features(df_future_site, df_site, required_union)
            spike_detail_available = True
        else:
            spike_detail_available = False
            detail_warn = "ë¯¸ë˜ ì…ë ¥ ë°ì´í„°(ê´€ì¸¡ì†Œ)ê°€ ì—†ê±°ë‚˜, ëª¨ë¸ í”¼ì²˜ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì–´ ì›ì¸ ë¶„ì„ì„ ìƒëµí•©ë‹ˆë‹¤."

        if model_errors:
            # ëª¨ë¸ ì¼ë¶€ë§Œ ì‹¤íŒ¨í•´ë„, ì„±ê³µí•œ ëª¨ë¸ì€ ì“¸ ìˆ˜ ìˆìœ¼ë‹ˆ warningë§Œ
            detail_warn = (detail_warn + " / " if detail_warn else "") + f"ëª¨ë¸ ì¼ë¶€ ë¡œë“œ ì‹¤íŒ¨: {', '.join(model_errors)}"

    except FileNotFoundError as e:
        spike_detail_available = False
        detail_warn = f"ë¯¸ë˜ ì…ë ¥ CSVê°€ ì—†ì–´ ìŠ¤íŒŒì´í¬/ì›ì¸ ë¶„ì„ì„ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤. ({e})"
    except Exception as e:
        spike_detail_available = False
        detail_warn = f"ë¯¸ë˜ ì…ë ¥/ëª¨ë¸ ì²˜ë¦¬ ì‹¤íŒ¨ â†’ ì›ì¸ ë¶„ì„ ë¹„í™œì„±í™”. ({type(e).__name__}: {e})"

    if detail_warn:
        st.caption(detail_warn)

    # ------------------------------
    # View controls
    # ------------------------------
    min_d = pd.to_datetime(df_site["date"].min())
    max_d_hist = pd.to_datetime(df_site["date"].max())

    # âœ… siteì˜ prophet ì˜ˆì¸¡ ë ë‚ ì§œ(ì—†ìœ¼ë©´ ê³¼ê±° max)
    pred_end = max_d_hist
    for (_site, _y), _pred in ALL_PROPHET.items():
        if str(_site) == str(site):
            try:
                pred_end = max(pred_end, pd.to_datetime(_pred["date"].max()))
            except Exception:
                pass

    # âœ… 2í˜ì´ì§€ ì²« ì§„ì…: ì „ì²´ ê¸°ê°„(2018 ~ pred_end) ë³´ì´ê²Œ
    default_start = min_d
    default_end = pred_end

    show_5months = st.toggle("ìµœê·¼ 5ê°œì›” ì‹œê³„ì—´ ë³´ê¸°", value=False)

    # âœ… date_input ìì²´ë„ pred_endê¹Œì§€ ì„ íƒ ê°€ëŠ¥í•´ì•¼ í•¨
    dr = st.date_input(
        "ğŸ“† ë‚ ì§œ ë²”ìœ„ ì„ íƒ (ê·¸ë˜í”„ ì´ë™)",
        value=(default_start.date(), default_end.date()),
        min_value=min_d.date(),
        max_value=default_end.date(),  # â­ ì—¬ê¸° ì¤‘ìš”
    )

    if isinstance(dr, tuple) and len(dr) == 2:
        view_start = pd.to_datetime(dr[0])
        view_end = pd.to_datetime(dr[1])
    else:
        single = pd.to_datetime(dr)
        view_start = single - pd.Timedelta(days=7)
        view_end = single + pd.Timedelta(days=7)

    # âœ… clampë„ pred_end ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼ ë¯¸ë˜ê°€ ì•ˆ ì˜ë¦¼
    view_start = max(view_start, min_d)
    view_end = min(view_end, default_end)

    if view_start > view_end:
        view_start, view_end = view_end, view_start

    # âœ… ìµœê·¼ 5ê°œì›” í† ê¸€: ê³¼ê±° 5ê°œì›” ~ pred_end(= 1/7ê¹Œì§€)
    if show_5months:
        view_start = max(min_d, anchor - pd.DateOffset(months=5))
        view_end = default_end

    view_range = (pd.to_datetime(view_start), pd.to_datetime(view_end))

    # ------------------------------
    # ìƒë‹¨ ìš”ì•½(í‘œì‹œìš©): SPIKE_DF ê¸°ë°˜
    # ------------------------------
    st.info(
        f"**ìš”ì•½:** ë³¸ ê´€ì¸¡ì†ŒëŠ” í˜„ì¬ **{site_cluster.upper()}** ìƒíƒœì´ë©°, "
        f"í–¥í›„ {horizon}ì¼ ì¤‘ **{n_spike_summary}ë²ˆ** ìŠ¤íŒŒì´í¬ ìœ„í—˜ì´ ì˜ˆì¸¡ë©ë‹ˆë‹¤ "
        f"(ìš°ì„ ìˆœìœ„: **{priority_summary}**)"
    )


    # ------------------------------
    # Main layout (ì›ë³¸ ìœ ì§€: ì¢Œ 2x2 / ìš° AI ë³´ê³ ì„œ)
    # ------------------------------
    left, right = st.columns([2, 1], gap="large")

    # ========== LEFT: 2x2 charts ==========
    with left:
        items = list(TARGETS.items())
        for row in [items[:2], items[2:]]:
            c1, c2 = st.columns(2, gap="medium")

            for (label, y_col), col in zip(row, [c1, c2]):
                with col:
                    st.markdown(f"**{label}**")

                    pred = ALL_PROPHET.get((str(site), y_col))
                    if pred is None:
                        st.info("ì˜ˆì¸¡ ë°ì´í„° ì—†ìŒ")
                        continue

                    # --- spike shading days
                    spike_days: List[pd.Timestamp] = []
                    top_risk_days: List[pd.Timestamp] = []  # âœ… ì„ê³„ì¹˜ ëª» ë„˜ì–´ë„ â€œìœ„í—˜ ìƒìœ„â€ ì œê³µ
                    top_risk_probs: List[float] = []

                    if spike_detail_available:
                        model = models.get(label, DummySpikeModel())
                        feats = _get_model_features(model)

                        if feats and (not getattr(model, "_is_dummy", False)) and (not df_future_site_filled.empty):
                            X = df_future_site_filled[feats]
                            probs = model.predict_proba(X)[:, 1]

                            # ì„ê³„ì¹˜ ë„˜ëŠ” ë‚ (ìŠ¤íŒŒì´í¬)
                            spike_days = df_future_site_filled.loc[probs >= SPIKE_THRESHOLD, "date"].tolist()

                            # âœ… ì„ê³„ì¹˜ ë¯¸ë§Œì´ì–´ë„ â€œìœ„í—˜ ìƒìœ„ 3ì¼â€ ë½‘ê¸° (ì›ì¸ë¶„ì„ ë¹„ì–´ìˆì§€ ì•Šê²Œ)
                            order = np.argsort(-probs)  # desc
                            k = min(3, len(order))
                            if k > 0:
                                idxs = order[:k]
                                top_risk_days = [pd.to_datetime(df_future_site_filled.iloc[i]["date"]) for i in idxs]
                                top_risk_probs = [float(probs[i]) for i in idxs]

                    fig = get_ts_figure_cached(
                        site=str(site),
                        y_col=y_col,
                        show_5months=show_5months,
                        view_start_iso=str(view_range[0]),
                        view_end_iso=str(view_range[1]),
                        anchor_iso=str(anchor),
                        spike_days_iso=tuple([str(pd.to_datetime(d)) for d in spike_days]),
                    )

                    if fig is not None:
                        st.plotly_chart(fig, use_container_width=True, config=PLOTLY_CFG)
                    else:
                        fig2 = build_ts_figure(
                            pred=pred,
                            df_all=df_all,
                            y_col=y_col,
                            anchor=anchor,
                            spike_days=spike_days,
                            show_5months=show_5months,
                            view_range=view_range,
                        )
                        st.plotly_chart(fig2, use_container_width=True, config=PLOTLY_CFG)

                    # --- Cause analysis
                    with st.expander("ğŸ” ëª¨ë¸ íŒë‹¨ì— ê¸°ì—¬í•œ ìš”ì¸", expanded=False):
                        if not spike_detail_available:
                            st.caption("ë¯¸ë˜ ì…ë ¥/ëª¨ë¸ì´ ì—†ì–´ì„œ ì›ì¸ ë¶„ì„ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        else:
                            model = models.get(label, DummySpikeModel())
                            feats = _get_model_features(model)

                            if not feats or getattr(model, "_is_dummy", False):
                                st.caption("ëª¨ë¸ í”¼ì²˜ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ì–´ ì›ì¸ ë¶„ì„ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.")
                            else:
                                X_all = df_future_site_filled[feats]
                                date_series = df_future_site_filled["date"]

                                # 1) ìŠ¤íŒŒì´í¬ ë°œìƒì¼ ê¸°ì¤€
                                if spike_days:
                                    st.markdown("**ìŠ¤íŒŒì´í¬ ë°œìƒ êµ¬ê°„ ê¸°ì¤€(ì´ë²¤íŠ¸) íŒë‹¨ ìš”ì¸:**")
                                    for d in spike_days[:3]:
                                        reason = explain_weather_keyword_event(
                                            model_key=label,
                                            model=model,
                                            X_all=X_all,
                                            dates=date_series,
                                            center_date=pd.to_datetime(d),
                                            window=1,
                                        )
                                        st.markdown(
                                            f"- **{pd.to_datetime(d).strftime('%Y-%m-%d')}**: "
                                            f"{reason or 'ê¸°ìƒ ìš”ì¸ ì˜í–¥ ë¯¸ì•½'}"
                                        )
                                        # ===============================
                                        # ğŸ“ˆ SHAP ê²°ê³¼ ê¸°ë°˜ ë¯¸ë˜ ê¸°ìƒ ê·¸ë˜í”„ (7ì¼)
                                        # ===============================
                                        if reason in SHAP_REASON_TO_COLUMN:
                                            col, label_kr = SHAP_REASON_TO_COLUMN[reason]

                                            if col in df_future_site_filled.columns:
                                                with st.expander(f"ğŸ“ˆ {label_kr} 7ì¼ ì˜ˆë³´", expanded=False):
                                                    df_view = (
                                                        df_future_site_filled
                                                        .set_index("date")[[col]]
                                                        .rename(columns={col: label_kr})
                                                    )

                                                    st.line_chart(df_view)
                                            else:
                                                st.caption(f"{label_kr} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                                # 2) ìŠ¤íŒŒì´í¬ ì—†ìœ¼ë©´ ìœ„í—˜ ìƒìœ„ì¼ ê¸°ì¤€
                                else:
                                    if not top_risk_days:
                                        st.markdown("ìŠ¤íŒŒì´í¬ ìœ„í—˜ì´ ë§¤ìš° ë‚®ê±°ë‚˜ ì˜ˆì¸¡ í™•ë¥ ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                    else:
                                        st.markdown("**ìŠ¤íŒŒì´í¬ëŠ” ì—†ìœ¼ë‚˜, ìœ„í—˜ ìƒìœ„ êµ¬ê°„ ê¸°ì¤€ íŒë‹¨ ìš”ì¸:**")
                                        for d, p in zip(top_risk_days, top_risk_probs):
                                            reason = explain_weather_keyword_event(
                                                model_key=label,
                                                model=model,
                                                X_all=X_all,
                                                dates=date_series,
                                                center_date=pd.to_datetime(d),
                                                window=1,
                                            )
                                            st.markdown(
                                                f"- **{pd.to_datetime(d).strftime('%Y-%m-%d')}** "
                                                f"(prob={p:.3f}): {reason or 'ê¸°ìƒ ìš”ì¸ ì˜í–¥ ë¯¸ì•½'}"
                                            )


    # ========== RIGHT: AI report (ì›ë³¸ ìœ ì§€) ==========
    with right:
        st.markdown("### ğŸ§  AI ìµœì¢… ë³´ê³ ì„œ")
        if not _HAS_OPENAI:
            st.warning("openai íŒ¨í‚¤ì§€ê°€ ì—†ì–´ ë³´ê³ ì„œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. (ëŒ€ì‹œë³´ë“œ í•µì‹¬ ê¸°ëŠ¥ì—ëŠ” ì˜í–¥ ì—†ìŒ)")

        report_style = st.selectbox("ë³´ê³ ì„œ í†¤", ["ìš´ì˜ììš©(ê°„ê²°)", "íŒ€ ë°œí‘œìš©(ì¡°ê¸ˆ ìì„¸íˆ)"], index=0)
        style_hint = (
            "ê° ì„¹ì…˜ì€ 3~6ì¤„ ë‚´ë¡œ ê°„ê²°í•˜ê²Œ. ì˜ì‚¬ê²°ì •ì— í•„ìš”í•œ ìˆ«ì/ë‚ ì§œ ìš°ì„ ."
            if report_style == "ìš´ì˜ììš©(ê°„ê²°)"
            else "ê° ì„¹ì…˜ì€ 5~10ì¤„. ê·¼ê±°(ë‚ ì§œ/í™•ë¥ /ì›ì¸)ë¥¼ 1ë¬¸ì¥ ë” í¬í•¨."
        )

        if "site_report_json" not in st.session_state:
            st.session_state["site_report_json"] = None  # type: ignore

        if st.button("ğŸ§  ë³´ê³ ì„œ ìƒì„±", use_container_width=True):
            pollutant_summaries: List[dict] = []

            for label in TARGETS:
                model = models.get(label, DummySpikeModel())
                feats = _get_model_features(model)

                if spike_detail_available and feats and (not getattr(model, "_is_dummy", False)) and (not df_future_site_filled.empty):
                    X = df_future_site_filled[feats]
                    probs = model.predict_proba(X)[:, 1]
                    order = np.argsort(-probs)
                    k = min(3, len(order))
                    top_idxs = order[:k] if k > 0 else []

                    spike_mask = probs >= SPIKE_THRESHOLD
                    spike_dates = df_future_site_filled.loc[spike_mask, "date"]

                    examples = []
                    for i in top_idxs:
                        d = pd.to_datetime(df_future_site_filled.iloc[i]["date"])
                        X_row = X[df_future_site_filled["date"] == d]
                        reason = explain_weather_keyword_event(
                            model_key = label,
                            model = model,
                            X_all = X,
                            dates = df_future_site_filled["date"],
                            center_date=d,
                            window=1) if not X_row.empty else None
                        examples.append(f"{d:%Y-%m-%d} (prob={float(probs[i]):.3f}) - driver={reason or 'N/A'}")

                    pollutant_summaries.append(
                        {
                            "label": label,
                            "n_spike": int(spike_mask.sum()),
                            "max_prob": float(np.max(probs)) if len(probs) else 0.0,
                            "mean_prob": float(np.mean(probs)) if len(probs) else 0.0,
                            "spike_examples": examples,
                        }
                    )
                else:
                    pollutant_summaries.append(
                        {
                            "label": label,
                            "n_spike": 0,
                            "max_prob": 0.0,
                            "mean_prob": 0.0,
                            "spike_examples": [],
                        }
                    )

            messages = build_llm_payload(site, anchor, horizon, pollutant_summaries, style_hint, spatial_context)
            try:
                with st.spinner("GPTê°€ ë³´ê³ ì„œë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    report_text = generate_report_from_llm(messages)
                st.session_state["site_report_json"] = safe_parse_report_json(report_text)
            except Exception as e:
                st.error(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")

        report_json: Optional[Dict[str, str]] = st.session_state.get("site_report_json")
        if report_json:
            with st.expander("ğŸ“Œ ë³´ê³ ì„œ ìš”ì•½", expanded=True):
                for idx, (title_kr, key) in enumerate(REPORT_SCHEMA_KEYS, start=1):
                    st.markdown(f"### {idx}. {title_kr}")
                    content = (report_json.get(key) or "").strip()
                    st.markdown(content if content else "_(ë‚´ìš© ì—†ìŒ)_")
        else:
            st.caption("ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì´ ì¹¸ì— ë³´ê³ ì„œê°€ ìƒì„±ë©ë‹ˆë‹¤.")
